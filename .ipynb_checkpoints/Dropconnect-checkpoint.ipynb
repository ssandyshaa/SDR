{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "import os, time, random, copy, csv\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import foolbox\n",
    "from data_helpers import *\n",
    "from general_helpers import *\n",
    "\n",
    "import argparse, os, time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from general_helpers import *\n",
    "from data_helpers import *\n",
    "from Adversary import AdvDataset, Generate_Adversarial_Samples\n",
    "from train_sdr import *\n",
    "from sdr_modules import *\n",
    "from resnet_sdr import *\n",
    "import torchnlp\n",
    "print ('loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "trainset, testset, splited = split_MNIST(train_val_split=True, save_path=None)\n",
    "train_loader, val_loader, _ = load_MNIST(64, trainset, testset, splited, 0)\n",
    "print ('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropconnect layer\n",
    "def _weight_drop(module, weights, dropout):\n",
    "    \"\"\"\n",
    "    Helper for `WeightDrop`.\n",
    "    \"\"\"\n",
    "    for name_w in weights:\n",
    "        w = getattr(module, name_w)\n",
    "        del module._parameters[name_w]\n",
    "        module.register_parameter(name_w + '_raw', Parameter(w))\n",
    "\n",
    "    original_module_forward = module.forward\n",
    "\n",
    "    def forward(*args, **kwargs):\n",
    "        for name_w in weights:\n",
    "            raw_w = getattr(module, name_w + '_raw')\n",
    "            w = torch.nn.functional.dropout(raw_w, p=dropout, training=module.training)\n",
    "            setattr(module, name_w, w)\n",
    "\n",
    "        return original_module_forward(*args)\n",
    "\n",
    "    setattr(module, 'forward', forward)\n",
    "    \n",
    "class WeightDropLinear(torch.nn.Linear):\n",
    "    \"\"\"\n",
    "    Wrapper around :class:`torch.nn.Linear` that adds ``weight_dropout`` named argument.\n",
    "\n",
    "    Args:\n",
    "        weight_dropout (float): The probability a weight will be dropped.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, weight_dropout=0.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        weights = ['weight']\n",
    "        _weight_drop(self, weights, weight_dropout)\n",
    "        \n",
    "class DC_MLP(nn.Module):\n",
    "    def __init__(self, hiddens, num_classes):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(len(hiddens) - 1):\n",
    "            l = WeightDropLinear(hiddens[i], hiddens[i+1], weight_dropout = 0.2)\n",
    "#             nn.init.xavier_normal_(l.weight.data)\n",
    "            layers.append(l)\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hiddens[-1], num_classes))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 28*28)\n",
    "        return self.layers(x)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hiddens, num_classes):\n",
    "        super().__init__()\n",
    "        self.sdr = False\n",
    "        layers = []\n",
    "        for i in range(len(hiddens) - 1):\n",
    "            l = nn.Linear(hiddens[i], hiddens[i+1])\n",
    "            nn.init.xavier_normal_(l.weight.data)\n",
    "            layers.append(l)\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hiddens[-1], num_classes))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 28*28)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, curr_epoch, device, std_init_fn = init_std_halved_xavier):\n",
    "    losses = AverageMeter(None)\n",
    "    accuracies = AverageMeter(None)\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        correct, batch_size  = accuracy(outputs, labels)\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        accuracies.update(100.0 * correct / batch_size, inputs.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        del inputs\n",
    "        del labels\n",
    "        #del store\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('[%d, %5d] loss: %.3f, accuracy: %.3f' % (curr_epoch+1, i+1, losses.avg, accuracies.avg))\n",
    "\n",
    "    torch.save(model.state_dict(), fname)\n",
    "\n",
    "    return losses.avg, accuracies.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "class softCrossEntropy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(softCrossEntropy, self).__init__()\n",
    "        return\n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "        \"\"\"\n",
    "        :param inputs: predictions\n",
    "        :param target: target labels\n",
    "        :return: loss\n",
    "        \"\"\"\n",
    "        log_likelihood = - F.log_softmax(inputs, dim=1)\n",
    "        sample_num, class_num = target.shape\n",
    "        loss = torch.sum(torch.mul(log_likelihood, target))/sample_num\n",
    "\n",
    "        return loss\n",
    "criterion = softCrossEntropy()\n",
    "print ('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "hiddens = [28*28, 100, 100, 100]\n",
    "num_classes = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 1e-4\n",
    "wdecay = 1.2e-6\n",
    "epochs = 15\n",
    "device = 'cuda'\n",
    "batch_size = 64\n",
    "model = DC_MLP(hiddens, num_classes)\n",
    "model_name = 'DC_model'\n",
    "model.load_state_dict(torch.load(model_name))\n",
    "# model = MLP(hiddens, num_classes)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model.cuda()\n",
    "trainset, testset, splited = split_MNIST(train_val_split=True, save_path=None)\n",
    "train_loader, val_loader, _ = load_MNIST(batch_size, trainset, testset, splited, 0)\n",
    "print ('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses = AverageMeter(None)\n",
    "# accuracies = AverageMeter(None)\n",
    "# model.train()\n",
    "# for e in range(epochs):\n",
    "#     for i, (inputs, labels) in enumerate(train_loader):\n",
    "\n",
    "#         inputs = inputs.to(device)\n",
    "#         labels = labels.to(device)\n",
    "\n",
    "#         outputs = model(inputs)\n",
    "\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         correct, batch_size  = accuracy(outputs, labels)\n",
    "#         losses.update(loss.item(), inputs.size(0))\n",
    "#         accuracies.update(100.0 * correct / batch_size, inputs.size(0))\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         del inputs\n",
    "#         del labels\n",
    "#         #del store\n",
    "#         torch.cuda.empty_cache()\n",
    "#     print('[%d, %5d] loss: %.3f, accuracy: %.3f' % (e+1, i+1, losses.avg, accuracies.avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'DC_model'\n",
    "# torch.save(model.state_dict(), model_name)\n",
    "# print ('saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running each sample on the stochastic model for 10 times.\n",
      "Classification accuracy 98.400\n"
     ]
    }
   ],
   "source": [
    "_ = validate_stochastic(val_loader, model, nn.CrossEntropyLoss().cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to generate new samples\n",
    "# method = 'DeepFool'\n",
    "# adv_samples, adv_tgts = Generate_Adversarial_Samples(model, val_loader, num_classes, method) #Generate Adv Samples\n",
    "# np.save('adv_samples/DC_val_adv_samples_MINIST_{}.npy'.format(method), adv_samples) #Save the adversarial samples\n",
    "# np.save('adv_samples/DC_val_adv_targets_MINIST_{}.npy'.format(method), adv_tgts) \n",
    "# print ('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1849\n"
     ]
    }
   ],
   "source": [
    "# Run this if samples have been generated before\n",
    "adv_samples_gradient = np.load('adv_samples/DC_val_adv_samples_MINIST_DeepFool.npy')\n",
    "adv_tgts_gradient = np.load('adv_samples/DC_val_adv_targets_MINIST_DeepFool.npy')\n",
    "print(len(adv_tgts_gradient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running each sample on the stochastic model for 1 times.\n",
      "Classification accuracy 42.942\n",
      "Running each sample on the stochastic model for 2 times.\n",
      "Classification accuracy 42.509\n",
      "Running each sample on the stochastic model for 5 times.\n",
      "Classification accuracy 42.455\n",
      "Running each sample on the stochastic model for 10 times.\n",
      "Classification accuracy 43.267\n"
     ]
    }
   ],
   "source": [
    "adv_dataset_gradient = AdvDataset(adv_samples_gradient, adv_tgts_gradient, 10)\n",
    "adv_dataloader_gradient = DataLoader(adv_dataset_gradient, batch_size = 64, num_workers = 4, drop_last = False)\n",
    "for x in [1,2,5,10]:\n",
    "    _ = validate_stochastic(adv_dataloader_gradient, model, nn.CrossEntropyLoss(), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adv_samples_regular, adv_tgts_regular = Generate_Adversarial_Samples(model, val_loader, num_classes) #Generate Adv Samples\n",
    "# np.save('DC_FSGM_val_adv_samples_MINIST.npy', adv_samples_regular) #Save the adversarial samples\n",
    "# np.save('val_FSGM_adv_targets_MINIST.npy', adv_tgts_regular) \n",
    "# print ('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running each sample on the stochastic model for 1 times.\n",
      "Classification accuracy 42.742\n",
      "Running each sample on the stochastic model for 2 times.\n",
      "Classification accuracy 45.907\n",
      "Running each sample on the stochastic model for 5 times.\n",
      "Classification accuracy 42.592\n",
      "Running each sample on the stochastic model for 10 times.\n",
      "Classification accuracy 44.952\n"
     ]
    }
   ],
   "source": [
    "adv_samples_regular = np.load('DC_FSGM_val_adv_samples_MINIST.npy')\n",
    "adv_tgts_regular = np.load('val_FSGM_adv_targets_MINIST.npy')\n",
    "adv_dataset_gradient = AdvDataset(adv_samples_regular, adv_tgts_regular, 10)\n",
    "adv_dataloader_gradient = DataLoader(adv_dataset_gradient, batch_size = 64, num_workers = 4, drop_last = False)\n",
    "for x in [1,2,5,10]:\n",
    "    _ = validate_stochastic(adv_dataloader_gradient, model, nn.CrossEntropyLoss(), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
